{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Design\n",
    "\n",
    "Knowledge gap between public and specialists and uncertainties are an important factors that drive pandemic anxiety, in this task, we will examine papers that discuss some of the controversial topics that contribute to rumours and anxiety\n",
    "\n",
    "Here I develop a search system to extract sentences from abstracts that are relevant to a question. The questions are associated with rumour and uncertain information circulating in the public. We can try different questions in here, and an important part is to evaluate the search system with human annotation baseline if we want to push forward this work as a paper. \n",
    "\n",
    "Step 1:\n",
    "The search system first extract abstract contains a keyword (e.g. ‘mask’), then we use LDA to group the abstract topics. We identify a topic that is  most relevant to the question and we extract abstracts that contain the target topic. The system sentences that contain the keyword from the relevant abstracts. The standard apporach of a search system is to used TFIDF to rank documents, here we use LDA topic modeling on nouns, verbs and adjectives of the abstract. Users can decide the relevant information when they know what are the most frequent keywords in each topic.\n",
    "\n",
    "Step 2:\n",
    "We manually annotate the key sentences to identify information in these key sentences. \n",
    "\n",
    "Keywords:\n",
    "Incubation period, asymptomatic, mask, death rate, paracetamone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    "import string\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "import spacy,en_core_web_sm\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read metadata into dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaData:\n",
    "    def __init__(self):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        # path and data\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.meta_data = pd.read_csv(self.path + 'metadata.csv')\n",
    "\n",
    "    def data_dict(self):\n",
    "        \"\"\"Convert df to dictionary. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        meta_data_dict = mydict()\n",
    "\n",
    "        for cord_uid, abstract, title, sha in zip(self.meta_data['cord_uid'], self.meta_data['abstract'], self.meta_data['title'], self.meta_data['sha']):\n",
    "            meta_data_dict[cord_uid]['title'] = title\n",
    "            meta_data_dict[cord_uid]['abstract'] = abstract\n",
    "            meta_data_dict[cord_uid]['sha'] = sha\n",
    "\n",
    "        return meta_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract documents contain keywords, preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractText:\n",
    "    \"\"\"Extract text according to keywords or phrases\"\"\"\n",
    "\n",
    "    def __init__(self, metaDict, keyword, variable):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.metadata = metaDict\n",
    "        self.keyword = keyword\n",
    "        self.variable = variable\n",
    "\n",
    "\n",
    "    def simple_preprocess(self):\n",
    "        \"\"\"Simple text process: lower case, remove punc. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cleaned = mydict()\n",
    "        for k, v in self.metadata.items():\n",
    "            sent = v[self.variable]\n",
    "            sent = str(sent).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            cleaned[k]['processed_text'] = sent\n",
    "            cleaned[k]['sha'] = v['sha']\n",
    "            cleaned[k]['title'] = v['title']\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def very_simple_preprocess(self):\n",
    "        \"\"\"Simple text process: lower case only. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cleaned = mydict()\n",
    "        for k, v in self.metadata.items():\n",
    "            sent = v[self.variable]\n",
    "            sent = str(sent).lower()\n",
    "            cleaned[k]['processed_text'] = sent\n",
    "            cleaned[k]['sha'] = v['sha']\n",
    "            cleaned[k]['title'] = v['title']\n",
    "\n",
    "        return cleaned\n",
    "     \n",
    "\n",
    "    def extract_w_keywords(self):\n",
    "        \"\"\"Select content with keywords.\"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        selected = mydict()\n",
    "        textdict = self.simple_preprocess()\n",
    "        for k, v in textdict.items():\n",
    "            if self.keyword in v['processed_text'].split():\n",
    "                #print(v['sha'])\n",
    "                selected[k]['processed_text'] = v['processed_text']\n",
    "                selected[k]['sha'] = v['sha']\n",
    "                selected[k]['title'] = v['title']\n",
    "        return selected\n",
    "\n",
    "    def extract_w_keywords_punc(self):\n",
    "        \"\"\"Select content with keywords, with punctuations in text\"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        selected = mydict()\n",
    "        textdict = self.very_simple_preprocess()\n",
    "        for k, v in textdict.items():\n",
    "            if self.keyword in v['processed_text'].split():\n",
    "                    #print(v['sha'])\n",
    "                selected[k]['processed_text'] = v['processed_text']\n",
    "                selected[k]['sha'] = v['sha']\n",
    "                selected[k]['title'] = v['title']\n",
    "        return selected\n",
    "\n",
    "    def get_noun_verb(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for LDA\"\"\"\n",
    "        ps = PorterStemmer()\n",
    "      \n",
    "        #find nound trunks\n",
    "        nlp = en_core_web_sm.load()\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            #v = v.replace('incubation period', 'incubation_period')\n",
    "            doc = nlp(v)\n",
    "            nouns = ' '.join(str(v) for v in doc if v.pos_ is 'NOUN').split()\n",
    "            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n",
    "            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n",
    "            all_w = nouns + verbs + adj\n",
    "            all_extracted[k] = all_w\n",
    "      \n",
    "        return all_extracted\n",
    "\n",
    "    def get_noun_verb2(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for LDA\"\"\"\n",
    "        ps = PorterStemmer()\n",
    "      \n",
    "        #find nound trunks\n",
    "        nlp = en_core_web_sm.load()\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            #v = v.replace('incubation period', 'incubation_period')\n",
    "            doc = nlp(v['processed_text'])\n",
    "            nouns = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'NOUN').split()\n",
    "            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n",
    "            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n",
    "            all_w = nouns + verbs + adj\n",
    "            all_extracted[k] = all_w\n",
    "      \n",
    "        return all_extracted\n",
    "\n",
    "    def tokenization(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for the next step\"\"\"\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            doc = nlp(v)\n",
    "            all_extracted[k] = [w.text for w in doc]\n",
    "      \n",
    "        return all_extracted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LDA to rank documents\n",
    "LDA is optimized by coherence score u_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDATopic:\n",
    "    def __init__(self, processed_text, topic_num, alpha, eta):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.text = processed_text\n",
    "        self.topic_num = topic_num\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "\n",
    "    def get_lda_score_eval(self, dictionary, bow_corpus):\n",
    "        \"\"\"LDA model and coherence score.\"\"\"\n",
    "\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=self.topic_num, id2word=dictionary, passes=10,  update_every=1, random_state = 300, alpha=self.alpha, eta=self.eta)\n",
    "        #pprint(lda_model.print_topics())\n",
    "\n",
    "        # get coherence score\n",
    "        cm = CoherenceModel(model=lda_model, corpus=bow_corpus, coherence='u_mass')\n",
    "        coherence = cm.get_coherence()\n",
    "        print('coherence score is {}'.format(coherence))\n",
    "\n",
    "        return lda_model, coherence\n",
    "\n",
    "    def get_score_dict(self, bow_corpus, lda_model_object):\n",
    "        \"\"\"\n",
    "        get lda score for each document\n",
    "        \"\"\"\n",
    "        all_lda_score = {}\n",
    "        for i in range(len(bow_corpus)):\n",
    "            lda_score ={}\n",
    "            for index, score in sorted(lda_model_object[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "                lda_score[index] = score\n",
    "                od = collections.OrderedDict(sorted(lda_score.items()))\n",
    "            all_lda_score[i] = od\n",
    "        return all_lda_score\n",
    "\n",
    "\n",
    "    def topic_modeling(self):\n",
    "        \"\"\"Get LDA topic modeling.\"\"\"\n",
    "        # generate dictionary\n",
    "        dictionary = gensim.corpora.Dictionary(self.text.values())\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in self.text.values()]\n",
    "        # modeling\n",
    "        model, coherence = self.get_lda_score_eval(dictionary, bow_corpus)\n",
    "\n",
    "        lda_score_all = self.get_score_dict(bow_corpus, model)\n",
    "\n",
    "        all_lda_score_df = pd.DataFrame.from_dict(lda_score_all)\n",
    "        all_lda_score_dfT = all_lda_score_df.T\n",
    "        all_lda_score_dfT = all_lda_score_dfT.fillna(0)\n",
    "\n",
    "        return model, coherence, all_lda_score_dfT\n",
    "\n",
    "    def get_ids_from_selected(self, text):\n",
    "        \"\"\"Get unique id from text \"\"\"\n",
    "        id_l = []\n",
    "        for k, v in text.items():\n",
    "            id_l.append(k)\n",
    "            \n",
    "        return id_l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select document (abstract/ article body) according to search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchArticleBody:\n",
    "    def __init__(self, path, selected_id):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = path\n",
    "        self.selected_id = selected_id\n",
    "\n",
    "\n",
    "    def read_folder(self):\n",
    "        \"\"\"\n",
    "        Creates a nested dictionary that represents the folder structure of rootdir\n",
    "        \"\"\"\n",
    "        rootdir = self.path.rstrip(os.sep)\n",
    "\n",
    "        article_dict = {}\n",
    "        for path, dirs, files in os.walk(rootdir):\n",
    "            for f in files:\n",
    "                file_id = f.split('.')[0]\n",
    "                #print(file_id)\n",
    "                try:\n",
    "                # load json file according to id\n",
    "                    with open(self.path + f) as f:\n",
    "                        data = json.load(f)\n",
    "                except:\n",
    "                    pass\n",
    "                article_dict[file_id] = data\n",
    "\n",
    "        return article_dict\n",
    "\n",
    "\n",
    "    def extract_bodytext(self):\n",
    "        \"\"\"Unpack nested dictionary and extract body of the article\"\"\"\n",
    "        body = {}\n",
    "        article_dict = self.read_folder()\n",
    "        for k, v in article_dict.items():\n",
    "            strings = ''\n",
    "            prevString = ''\n",
    "            for entry in v['body_text']:\n",
    "                strings = strings + prevString\n",
    "                prevString = entry['text']\n",
    "\n",
    "            body[k] = strings\n",
    "        return body\n",
    "\n",
    "\n",
    "    def get_title_by_bodykv(self, article_dict, keyword):\n",
    "        \"\"\"Search keyword in article body and return title\"\"\"\n",
    "\n",
    "        article_dict = self.read_folder()\n",
    "        selected_id = self.extract_id_list()\n",
    "\n",
    "        result = {}\n",
    "        for k, v in article_dict.items():\n",
    "            for entry in v['body_text']:\n",
    "                if (keyword in entry['text'].split()) and (k in selected_id):\n",
    "                    result[k] = v['metadata']['title']\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def extract_id_list(self):\n",
    "        \"\"\"Extract ids from the selected text. \"\"\"\n",
    "        selected_id = []\n",
    "        for k, v in self.selected_id.items():\n",
    "            selected_id.append(str(v['sha']).split(';')[0])\n",
    "            try:\n",
    "                selected_id.append(str(v['sha']).split(';')[1])\n",
    "                selected_id.append(str(v['sha']).split(';')[2])\n",
    "                selected_id.append(str(v['sha']).split(';')[3])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return selected_id\n",
    "\n",
    "\n",
    "    def select_text_w_id(self):\n",
    "        body_text = self.extract_bodytext()\n",
    "        selected_id = self.extract_id_list()\n",
    "        selected_text = {}\n",
    "        for k, v in body_text.items():\n",
    "            if k in selected_id:\n",
    "                selected_text[k] = v\n",
    "        return selected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we extract articles contain the most relevant topic\n",
    "\n",
    "def selected_best_LDA(keyword, varname):\n",
    "        \"\"\"Select the best lda model with extracted text \"\"\"\n",
    "        # convert data to dictionary format\n",
    "        m = MetaData()\n",
    "        metaDict = m.data_dict()\n",
    "\n",
    "        #process text and extract text with keywords\n",
    "        et = ExtractText(metaDict, keyword, varname)\n",
    "        text1 = et.extract_w_keywords()\n",
    "\n",
    "\n",
    "        # extract nouns, verbs and adjetives\n",
    "        text = et.get_noun_verb2(text1)\n",
    "\n",
    "        # optimized alpha and beta\n",
    "        alpha = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        beta = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cohere_dict = mydict()\n",
    "        for a in alpha:\n",
    "            for b in beta:\n",
    "                lda = LDATopic(text, 20, a, b)\n",
    "                model, coherence, scores = lda.topic_modeling()\n",
    "                cohere_dict[coherence]['a'] = a\n",
    "                cohere_dict[coherence]['b'] = b\n",
    "    \n",
    "        # sort result dictionary to identify the best a, b\n",
    "        # select a,b with the largest coherence score \n",
    "        sort = sorted(cohere_dict.keys())[0] \n",
    "        a = cohere_dict[sort]['a']\n",
    "        b = cohere_dict[sort]['b']\n",
    "        \n",
    "        # run LDA with the optimized values\n",
    "        lda = LDATopic(text, 20, a, b)\n",
    "        model, coherence, scores_best = lda.topic_modeling()\n",
    "        pprint(model.print_topics())\n",
    "\n",
    "        # select merge ids with the LDA topic scores\n",
    "        id_l = lda.get_ids_from_selected(text)\n",
    "        scores_best['cord_uid'] = id_l\n",
    "\n",
    "        return scores_best\n",
    "\n",
    "\n",
    "\n",
    "def select_text_from_LDA_results(keyword, varname, scores_best, topic_num):\n",
    "        # choose papers with the most relevant topic\n",
    "        # convert data to dictionary format\n",
    "        m = MetaData()\n",
    "        metaDict = m.data_dict()\n",
    "\n",
    "        # process text and extract text with keywords\n",
    "        et = ExtractText(metaDict, keyword, varname)\n",
    "        # extract text together with punctuation\n",
    "        text1 = et.extract_w_keywords_punc()\n",
    "        # need to decide which topic to choose after training\n",
    "        sel = scores_best[scores_best[topic_num] > 0] \n",
    "        \n",
    "\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        selected = mydict()\n",
    "        for k, v in text1.items():\n",
    "            if k in sel.cord_uid.tolist():\n",
    "                selected[k]['title'] = v['title']\n",
    "                selected[k]['processed_text'] = v['processed_text']\n",
    "                selected[k]['sha'] = v['sha']\n",
    "\n",
    "        return selected\n",
    "\n",
    "def extract_relevant_sentences(cor_dict, search_keywords):\n",
    "    \"\"\"Extract sentences contain keyword in relevant articles. \"\"\"\n",
    "\n",
    "    mydict = lambda: defaultdict(mydict)\n",
    "    sel_sentence = mydict()\n",
    "    \n",
    "    for k, v in cor_dict.items():\n",
    "        keyword_sentence = []\n",
    "        sentences = v['processed_text'].split('.')\n",
    "        for sentence in sentences:\n",
    "            # for each sentence, check if keyword exist\n",
    "            # append sentences contain keyword to list\n",
    "            keyword_sum = sum(1 for word in search_keywords if word in sentence)\n",
    "            if keyword_sum > 0:\n",
    "                keyword_sentence.append(sentence)\n",
    "            \n",
    "\n",
    "        # store results\n",
    "        sel_sentence[k]['sentences'] = keyword_sentence\n",
    "        sel_sentence[k]['sha'] = v['sha']\n",
    "        sel_sentence[k]['title'] = v['title']\n",
    "    print('{} articles are relevant to the topic you choose'.format(len(sel_sentence)))\n",
    "\n",
    "    path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "    df = pd.DataFrame.from_dict(sel_sentence, orient='index')\n",
    "    df.to_csv(path + 'search_results_{}.csv'.format(search_keywords))\n",
    "    sel_sentence_df = pd.read_csv(path + 'search_results_{}.csv'.format(search_keywords))\n",
    "    return sel_sentence, sel_sentence_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 Is wearing mask an effective way to control pandemic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence score is -4.336722639525242\n",
      "coherence score is -3.9010410239440367\n",
      "coherence score is -4.389157036678158\n",
      "coherence score is -4.585109650895275\n",
      "coherence score is -4.658858479495897\n",
      "coherence score is -4.181983373771045\n",
      "coherence score is -3.8225572164182773\n",
      "coherence score is -4.115948791902841\n",
      "coherence score is -4.405996653207033\n",
      "coherence score is -4.893575872869723\n",
      "coherence score is -3.763406611071013\n",
      "coherence score is -4.054636612549744\n",
      "coherence score is -3.974976478023649\n",
      "coherence score is -3.605098511028769\n",
      "coherence score is -3.218469889363284\n",
      "coherence score is -3.922771443759005\n",
      "coherence score is -4.56961216145051\n",
      "coherence score is -2.9539869626024027\n",
      "coherence score is -3.2776086868166603\n",
      "coherence score is -3.6310570980589993\n",
      "coherence score is -3.8129640801122164\n",
      "coherence score is -3.7657510362336026\n",
      "coherence score is -3.4720471247415454\n",
      "coherence score is -3.0704332896336615\n",
      "coherence score is -2.990024667297296\n",
      "coherence score is -4.893575872869723\n",
      "[(0,\n",
      "  '0.012*\"patient\" + 0.009*\"use\" + 0.009*\"recommend\" + 0.008*\"mask\" + '\n",
      "  '0.007*\"ventil\" + 0.006*\"acute\" + 0.005*\"respiratory\" + 0.004*\"failur\" + '\n",
      "  '0.004*\"niv\" + 0.003*\"intub\"'),\n",
      " (1,\n",
      "  '0.010*\"mask\" + 0.007*\"public\" + 0.007*\"infect\" + 0.006*\"use\" + 0.006*\"wear\" '\n",
      "  '+ 0.005*\"studi\" + 0.005*\"measur\" + 0.005*\"face\" + 0.005*\"behavior\" + '\n",
      "  '0.005*\"health\"'),\n",
      " (2,\n",
      "  '0.004*\"particl\" + 0.003*\"valu\" + 0.003*\"penetr\" + 0.003*\"size\" + '\n",
      "  '0.003*\"filter\" + 0.002*\"mask\" + 0.002*\"leak\" + 0.002*\"gauz\" + 0.002*\"model\" '\n",
      "  '+ 0.002*\"method\"'),\n",
      " (3,\n",
      "  '0.003*\"epitop\" + 0.002*\"protect\" + 0.002*\"vaccin\" + 0.002*\"skin\" + '\n",
      "  '0.001*\"immunogen\" + 0.001*\"neutral\" + 0.001*\"membran\" + 0.001*\"mucous\" + '\n",
      "  '0.001*\"design\" + 0.001*\"immune\"'),\n",
      " (4,\n",
      "  '0.004*\"viral\" + 0.003*\"psychological\" + 0.003*\"citi\" + 0.002*\"data\" + '\n",
      "  '0.002*\"such\" + 0.002*\"human\" + 0.002*\"impact\" + 0.002*\"virus\" + '\n",
      "  '0.002*\"sampl\" + 0.002*\"anxieti\"'),\n",
      " (5,\n",
      "  '0.011*\"mask\" + 0.008*\"air\" + 0.007*\"exhal\" + 0.006*\"cough\" + 0.005*\"oxygen\" '\n",
      "  '+ 0.005*\"smoke\" + 0.005*\"dispers\" + 0.004*\"patient\" + 0.004*\"leakag\" + '\n",
      "  '0.004*\"plume\"'),\n",
      " (6,\n",
      "  '0.026*\"mask\" + 0.010*\"infect\" + 0.009*\"use\" + 0.007*\"respir\" + '\n",
      "  '0.007*\"group\" + 0.007*\"respiratory\" + 0.006*\"protect\" + 0.005*\"medical\" + '\n",
      "  '0.005*\"control\" + 0.005*\"test\"'),\n",
      " (7,\n",
      "  '0.002*\"activ\" + 0.002*\"properti\" + 0.002*\"eo\" + 0.001*\"antimicrobial\" + '\n",
      "  '0.001*\"tgev\" + 0.001*\"acid\" + 0.001*\"mutant\" + 0.001*\"bind\" + '\n",
      "  '0.001*\"hemagglutin\" + 0.001*\"sialic\"'),\n",
      " (8,\n",
      "  '0.000*\"mask\" + 0.000*\"use\" + 0.000*\"air\" + 0.000*\"place\" + 0.000*\"acute\" + '\n",
      "  '0.000*\"patient\" + 0.000*\"travel\" + 0.000*\"respiratory\" + 0.000*\"studi\" + '\n",
      "  '0.000*\"infect\"'),\n",
      " (9,\n",
      "  '0.018*\"mask\" + 0.018*\"use\" + 0.013*\"patient\" + 0.011*\"infect\" + '\n",
      "  '0.009*\"respiratory\" + 0.006*\"measur\" + 0.006*\"transmiss\" + 0.006*\"studi\" + '\n",
      "  '0.006*\"care\" + 0.006*\"risk\"'),\n",
      " (10,\n",
      "  '0.016*\"air\" + 0.008*\"use\" + 0.007*\"cleaner\" + 0.006*\"inroom\" + 0.006*\"room\" '\n",
      "  '+ 0.005*\"technolog\" + 0.005*\"hepa\" + 0.005*\"infectious\" + 0.004*\"airborne\" '\n",
      "  '+ 0.004*\"uvgi\"'),\n",
      " (11,\n",
      "  '0.004*\"view\" + 0.003*\"obstruct\" + 0.002*\"assess\" + 0.001*\"level\" + '\n",
      "  '0.001*\"paramet\" + 0.001*\"simple\" + 0.001*\"sky\" + 0.001*\"premium\" + '\n",
      "  '0.001*\"unobstructed\" + 0.001*\"residential\"'),\n",
      " (12,\n",
      "  '0.003*\"model\" + 0.002*\"rat\" + 0.001*\"estim\" + 0.001*\"weight\" + 0.001*\"mice\" '\n",
      "  '+ 0.001*\"forecast\" + 0.001*\"encephalitis\" + 0.001*\"paramet\" + 0.001*\"error\" '\n",
      "  '+ 0.001*\"casepati\"'),\n",
      " (13,\n",
      "  '0.000*\"studi\" + 0.000*\"use\" + 0.000*\"mask\" + 0.000*\"capit\" + 0.000*\"social\" '\n",
      "  '+ 0.000*\"discoveri\" + 0.000*\"potential\" + 0.000*\"measur\" + 0.000*\"link\" + '\n",
      "  '0.000*\"reduc\"'),\n",
      " (14,\n",
      "  '0.005*\"household\" + 0.003*\"emerg\" + 0.003*\"prepared\" + 0.003*\"patient\" + '\n",
      "  '0.003*\"meningococcal\" + 0.002*\"wear\" + 0.002*\"adjust\" + 0.002*\"doctor\" + '\n",
      "  '0.002*\"consult\" + 0.002*\"primary\"'),\n",
      " (15,\n",
      "  '0.004*\"filter\" + 0.001*\"untreated\" + 0.001*\"surviv\" + 0.001*\"relative\" + '\n",
      "  '0.001*\"treat\" + 0.001*\"dialdehyd\" + 0.001*\"biocid\" + 0.001*\"inactiv\" + '\n",
      "  '0.001*\"viable\" + 0.001*\"effici\"'),\n",
      " (16,\n",
      "  '0.005*\"antibodi\" + 0.004*\"protein\" + 0.002*\"neutral\" + 0.002*\"sampl\" + '\n",
      "  '0.002*\"serum\" + 0.002*\"region\" + 0.002*\"antigen\" + 0.002*\"domain\" + '\n",
      "  '0.002*\"show\" + 0.002*\"immun\"'),\n",
      " (17,\n",
      "  '0.002*\"symptom\" + 0.002*\"physical\" + 0.002*\"flow\" + 0.002*\"stress\" + '\n",
      "  '0.002*\"psychological\" + 0.001*\"haze\" + 0.001*\"techniqu\" + 0.001*\"score\" + '\n",
      "  '0.001*\"total\" + 0.001*\"role\"'),\n",
      " (18,\n",
      "  '0.000*\"mask\" + 0.000*\"use\" + 0.000*\"infectious\" + 0.000*\"studi\" + '\n",
      "  '0.000*\"result\" + 0.000*\"measur\" + 0.000*\"method\" + 0.000*\"filter\" + '\n",
      "  '0.000*\"respiratory\" + 0.000*\"air\"'),\n",
      " (19,\n",
      "  '0.007*\"intervent\" + 0.006*\"studi\" + 0.005*\"influenza\" + 0.005*\"infect\" + '\n",
      "  '0.004*\"use\" + 0.004*\"contig\" + 0.004*\"rpe\" + 0.003*\"haplotyp\" + '\n",
      "  '0.003*\"data\" + 0.003*\"pandemic\"')]\n"
     ]
    }
   ],
   "source": [
    "#here we select the LDA model with the lowe\n",
    "scores_best = selected_best_LDA('mask', 'abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe topic No. 1 is most relevant to public wearing mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 abstracts selected\n"
     ]
    }
   ],
   "source": [
    "# topic number 1 is most relevant to public wearing mask\n",
    "# which topic do you think is most relevant to your search\n",
    "cor_dict = select_text_from_LDA_results('mask', 'abstract', scores_best, 1)\n",
    "print (\"There are {} abstracts selected\". format(len(cor_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 articles are relevant to the topic you choose\n"
     ]
    }
   ],
   "source": [
    "# extract relevant sentences  #search keywords can be a list\n",
    "sel_sentence, sel_sentence_df = extract_relevant_sentences(cor_dict, ['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sha</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8o3l3rsf</td>\n",
       "      <td>[', escalatory quarantine, mask wearing when g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Effectiveness of control strategies for Corona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1mu1z4xd</td>\n",
       "      <td>[' wearing a mask when going out and avoiding ...</td>\n",
       "      <td>5bb89950ec5a06e2b7f69b2a9c4213dda19b1ab0</td>\n",
       "      <td>Prediction of New Coronavirus Infection Based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ht88wu6s</td>\n",
       "      <td>[' conclusion: to early end of the covid-19 ep...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estimating the reproductive number and the out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nzh87aux</td>\n",
       "      <td>[' on the other hand, the model predicts that ...</td>\n",
       "      <td>9b7a0ad7b6c7f59e7a6cf1dc9d07912a273d19b5</td>\n",
       "      <td>The Waiting Time for Inter-Country Spread of P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n2r4pzan</td>\n",
       "      <td>[', wearing face mask in public venues (73', '...</td>\n",
       "      <td>b7c8e73cf095e30552a32cea04a398331c55ab41</td>\n",
       "      <td>Anticipated and current preventive behaviors i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ywb9krdp</td>\n",
       "      <td>['2%), and wear a face mask (59']</td>\n",
       "      <td>16627f4c7134394da448b1417a771d13ad7cca4a</td>\n",
       "      <td>Pandemic influenza in Australia: Using telepho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bhnh2dq4</td>\n",
       "      <td>[' if an infected person will not use a mask a...</td>\n",
       "      <td>bb9f6cef633c9baf595daae5166b11f88c1271cb</td>\n",
       "      <td>Risk of transmission of airborne infection dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>49xvz389</td>\n",
       "      <td>['3%) were carrying out one of prevention meas...</td>\n",
       "      <td>545def8771357b4cb2875f5795a0760e97534cc9</td>\n",
       "      <td>Knowledge and attitudes of university students...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>r3in76wm</td>\n",
       "      <td>[' preventive behavior such as handwashing and...</td>\n",
       "      <td>24d7fe6bbb9945f1536fef5b281d074fe69cfc6a</td>\n",
       "      <td>Avian Influenza Risk Perception and Preventive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e94synjc</td>\n",
       "      <td>['this research assessed factors associated wi...</td>\n",
       "      <td>14d04f36cb13550aa7769b61a079fa54031a21eb</td>\n",
       "      <td>Public health measures during an anticipated i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                          sentences  \\\n",
       "0   8o3l3rsf  [', escalatory quarantine, mask wearing when g...   \n",
       "1   1mu1z4xd  [' wearing a mask when going out and avoiding ...   \n",
       "2   ht88wu6s  [' conclusion: to early end of the covid-19 ep...   \n",
       "3   nzh87aux  [' on the other hand, the model predicts that ...   \n",
       "4   n2r4pzan  [', wearing face mask in public venues (73', '...   \n",
       "5   ywb9krdp                  ['2%), and wear a face mask (59']   \n",
       "6   bhnh2dq4  [' if an infected person will not use a mask a...   \n",
       "7   49xvz389  ['3%) were carrying out one of prevention meas...   \n",
       "8   r3in76wm  [' preventive behavior such as handwashing and...   \n",
       "9   e94synjc  ['this research assessed factors associated wi...   \n",
       "\n",
       "                                        sha  \\\n",
       "0                                       NaN   \n",
       "1  5bb89950ec5a06e2b7f69b2a9c4213dda19b1ab0   \n",
       "2                                       NaN   \n",
       "3  9b7a0ad7b6c7f59e7a6cf1dc9d07912a273d19b5   \n",
       "4  b7c8e73cf095e30552a32cea04a398331c55ab41   \n",
       "5  16627f4c7134394da448b1417a771d13ad7cca4a   \n",
       "6  bb9f6cef633c9baf595daae5166b11f88c1271cb   \n",
       "7  545def8771357b4cb2875f5795a0760e97534cc9   \n",
       "8  24d7fe6bbb9945f1536fef5b281d074fe69cfc6a   \n",
       "9  14d04f36cb13550aa7769b61a079fa54031a21eb   \n",
       "\n",
       "                                               title  \n",
       "0  Effectiveness of control strategies for Corona...  \n",
       "1  Prediction of New Coronavirus Infection Based ...  \n",
       "2  Estimating the reproductive number and the out...  \n",
       "3  The Waiting Time for Inter-Country Spread of P...  \n",
       "4  Anticipated and current preventive behaviors i...  \n",
       "5  Pandemic influenza in Australia: Using telepho...  \n",
       "6  Risk of transmission of airborne infection dur...  \n",
       "7  Knowledge and attitudes of university students...  \n",
       "8  Avian Influenza Risk Perception and Preventive...  \n",
       "9  Public health measures during an anticipated i...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read extracted article\n",
    "sel_sentence_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation \n",
    "We extracted 33 papers that are supposed to discuss whether using masks is useful. We annotate  whether the key sentences suggest using mask can reduce the risk of infection.\n",
    "\n",
    "Annotation \n",
    "1. ‘1’ sentences that support using a mask during a pandemic is useful \n",
    "2. ‘2’  papers that assume masks as useful and examine the public’s willingness to comply the rules,\n",
    "3. ’0’ no obvious evidence that shows using mask is protective or the protection is very little\n",
    "4. Not relevant to the above points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we need to add the stats analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "According to the key sentences in 33 abstract that discuss the topic of public using masks, only one paper suggests that there’s not enough evidence to show that mask is useful.\n",
    "There are 14 papers that suggest their results show using surgical mask during a pandemic is effective in reducing infection\n",
    "14 paper consider public individuals using masks are necessary in reducing risks of being infect, and these paper look at whether the public are willing to comply to the rules. (X papers are from  Hong Kong, based on the region of the first author)\n",
    "5 papers are not relevant to the topic\n",
    "\n",
    "Conclusion:\n",
    "government in some regions advocate using masks as a standard approach to reduce risk of infection, papers in these regions focus on whether people comply to the rules. When some government advocate that there is little evidence show that mask is effective in controlling the pandemic, nearly half of the academic papers from our search result either consider wearing masks as a standard practice that the public show comply, nearly half of the papers found evidence to support that wearing masks is effective in controlling the pandemic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
