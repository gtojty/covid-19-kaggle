{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Design\n",
    "\n",
    "Knowledge gap between public and specialists and uncertainties are an important factors that drive pandemic anxiety, in this task, we will examine papers that discuss some of the controversial topics that contribute to rumours and anxiety\n",
    "\n",
    "Here I develop a search system to extract sentences from abstracts that are relevant to a question. The questions are associated with rumour and uncertain information circulating in the public. We can try different questions in here, and an important part is to evaluate the search system with human annotation baseline if we want to push forward this work as a paper. \n",
    "\n",
    "Step 1:\n",
    "The search system first extract abstract contains a keyword (e.g. ‘mask’), then we use LDA to group the abstract topics. We identify a topic that is  most relevant to the question and we extract abstracts that contain the target topic. The system sentences that contain the keyword from the relevant abstracts. \n",
    "\n",
    "Step 2:\n",
    "We manually annotate the key sentences to identify information in these key sentences. \n",
    "\n",
    "Keywords:\n",
    "Incubation period, asymptomatic, mask, death rate, paracetamone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    "import string\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "import spacy,en_core_web_sm\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read metadata into dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaData:\n",
    "    def __init__(self):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        # path and data\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.meta_data = pd.read_csv(self.path + 'metadata.csv')\n",
    "\n",
    "    def data_dict(self):\n",
    "        \"\"\"Convert df to dictionary. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        meta_data_dict = mydict()\n",
    "\n",
    "        for cord_uid, abstract, title, sha in zip(self.meta_data['cord_uid'], self.meta_data['abstract'], self.meta_data['title'], self.meta_data['sha']):\n",
    "            meta_data_dict[cord_uid]['title'] = title\n",
    "            meta_data_dict[cord_uid]['abstract'] = abstract\n",
    "            meta_data_dict[cord_uid]['sha'] = sha\n",
    "\n",
    "        return meta_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract documents contain keywords, preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractText:\n",
    "    \"\"\"Extract text according to keywords or phrases\"\"\"\n",
    "\n",
    "    def __init__(self, metaDict, keyword, variable):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.metadata = metaDict\n",
    "        self.keyword = keyword\n",
    "        self.variable = variable\n",
    "\n",
    "\n",
    "    def simple_preprocess(self):\n",
    "        \"\"\"Simple text process: lower case, remove punc. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cleaned = mydict()\n",
    "        for k, v in self.metadata.items():\n",
    "            sent = v[self.variable]\n",
    "            sent = str(sent).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            cleaned[k]['processed_text'] = sent\n",
    "            cleaned[k]['sha'] = v['sha']\n",
    "            cleaned[k]['title'] = v['title']\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def very_simple_preprocess(self):\n",
    "        \"\"\"Simple text process: lower case only. \"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        cleaned = mydict()\n",
    "        for k, v in self.metadata.items():\n",
    "            sent = v[self.variable]\n",
    "            sent = str(sent).lower()\n",
    "            cleaned[k]['processed_text'] = sent\n",
    "            cleaned[k]['sha'] = v['sha']\n",
    "            cleaned[k]['title'] = v['title']\n",
    "\n",
    "        return cleaned\n",
    "     \n",
    "\n",
    "    def extract_w_keywords(self):\n",
    "        \"\"\"Select content with keywords.\"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        selected = mydict()\n",
    "        textdict = self.simple_preprocess()\n",
    "        for k, v in textdict.items():\n",
    "            if self.keyword in v['processed_text'].split():\n",
    "                #print(v['sha'])\n",
    "                selected[k]['processed_text'] = v['processed_text']\n",
    "                selected[k]['sha'] = v['sha']\n",
    "                selected[k]['title'] = v['title']\n",
    "        return selected\n",
    "\n",
    "    def extract_w_keywords_punc(self):\n",
    "        \"\"\"Select content with keywords, with punctuations in text\"\"\"\n",
    "        mydict = lambda: defaultdict(mydict)\n",
    "        selected = mydict()\n",
    "        textdict = self.very_simple_preprocess()\n",
    "        for k, v in textdict.items():\n",
    "            if self.keyword in v['processed_text'].split():\n",
    "                    #print(v['sha'])\n",
    "                selected[k]['processed_text'] = v['processed_text']\n",
    "                selected[k]['sha'] = v['sha']\n",
    "                selected[k]['title'] = v['title']\n",
    "        return selected\n",
    "\n",
    "    def get_noun_verb(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for LDA\"\"\"\n",
    "        ps = PorterStemmer()\n",
    "      \n",
    "        #find nound trunks\n",
    "        nlp = en_core_web_sm.load()\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            #v = v.replace('incubation period', 'incubation_period')\n",
    "            doc = nlp(v)\n",
    "            nouns = ' '.join(str(v) for v in doc if v.pos_ is 'NOUN').split()\n",
    "            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n",
    "            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n",
    "            all_w = nouns + verbs + adj\n",
    "            all_extracted[k] = all_w\n",
    "      \n",
    "        return all_extracted\n",
    "\n",
    "    def get_noun_verb2(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for LDA\"\"\"\n",
    "        ps = PorterStemmer()\n",
    "      \n",
    "        #find nound trunks\n",
    "        nlp = en_core_web_sm.load()\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            #v = v.replace('incubation period', 'incubation_period')\n",
    "            doc = nlp(v['processed_text'])\n",
    "            nouns = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'NOUN').split()\n",
    "            verbs = ' '.join(ps.stem(str(v)) for v in doc if v.pos_ is 'VERB').split()\n",
    "            adj = ' '.join(str(v) for v in doc if v.pos_ is 'ADJ').split()\n",
    "            all_w = nouns + verbs + adj\n",
    "            all_extracted[k] = all_w\n",
    "      \n",
    "        return all_extracted\n",
    "\n",
    "    def tokenization(self, text):\n",
    "        \"\"\"get noun trunks for the lda model,\n",
    "        change noun and verb part to decide what\n",
    "        you want to use as input for the next step\"\"\"\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        all_extracted = {}\n",
    "        for k, v in text.items():\n",
    "            doc = nlp(v)\n",
    "            all_extracted[k] = [w.text for w in doc]\n",
    "      \n",
    "        return all_extracted\n",
    "\n",
    "\n",
    "\n",
    "class LDATopic:\n",
    "    def __init__(self, processed_text, topic_num, alpha, eta):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = '/afs/inf.ed.ac.uk/user/s16/s1690903/share/cov19_2/'\n",
    "        self.text = processed_text\n",
    "        self.topic_num = topic_num\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "\n",
    "    def get_lda_score_eval(self, dictionary, bow_corpus):\n",
    "        \"\"\"LDA model and coherence score.\"\"\"\n",
    "\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=self.topic_num, id2word=dictionary, passes=10,  update_every=1, random_state = 300, alpha=self.alpha, eta=self.eta)\n",
    "        #pprint(lda_model.print_topics())\n",
    "\n",
    "        # get coherence score\n",
    "        cm = CoherenceModel(model=lda_model, corpus=bow_corpus, coherence='u_mass')\n",
    "        coherence = cm.get_coherence()\n",
    "        print('coherence score is {}'.format(coherence))\n",
    "\n",
    "        return lda_model, coherence\n",
    "\n",
    "    def get_score_dict(self, bow_corpus, lda_model_object):\n",
    "        \"\"\"\n",
    "        get lda score for each document\n",
    "        \"\"\"\n",
    "        all_lda_score = {}\n",
    "        for i in range(len(bow_corpus)):\n",
    "            lda_score ={}\n",
    "            for index, score in sorted(lda_model_object[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "                lda_score[index] = score\n",
    "                od = collections.OrderedDict(sorted(lda_score.items()))\n",
    "            all_lda_score[i] = od\n",
    "        return all_lda_score\n",
    "\n",
    "\n",
    "    def topic_modeling(self):\n",
    "        \"\"\"Get LDA topic modeling.\"\"\"\n",
    "        # generate dictionary\n",
    "        dictionary = gensim.corpora.Dictionary(self.text.values())\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in self.text.values()]\n",
    "        # modeling\n",
    "        model, coherence = self.get_lda_score_eval(dictionary, bow_corpus)\n",
    "\n",
    "        lda_score_all = self.get_score_dict(bow_corpus, model)\n",
    "\n",
    "        all_lda_score_df = pd.DataFrame.from_dict(lda_score_all)\n",
    "        all_lda_score_dfT = all_lda_score_df.T\n",
    "        all_lda_score_dfT = all_lda_score_dfT.fillna(0)\n",
    "\n",
    "        return model, coherence, all_lda_score_dfT\n",
    "\n",
    "    def get_ids_from_selected(self, text):\n",
    "        \"\"\"Get unique id from text \"\"\"\n",
    "        id_l = []\n",
    "        for k, v in text.items():\n",
    "            id_l.append(k)\n",
    "            \n",
    "        return id_l\n",
    "\n",
    "\n",
    "\n",
    "class MatchArticleBody:\n",
    "    def __init__(self, path, selected_id):\n",
    "        \"\"\"Define varibles.\"\"\"\n",
    "        self.path = path\n",
    "        self.selected_id = selected_id\n",
    "\n",
    "\n",
    "    def read_folder(self):\n",
    "        \"\"\"\n",
    "        Creates a nested dictionary that represents the folder structure of rootdir\n",
    "        \"\"\"\n",
    "        rootdir = self.path.rstrip(os.sep)\n",
    "\n",
    "        article_dict = {}\n",
    "        for path, dirs, files in os.walk(rootdir):\n",
    "            for f in files:\n",
    "                file_id = f.split('.')[0]\n",
    "                #print(file_id)\n",
    "                try:\n",
    "                # load json file according to id\n",
    "                    with open(self.path + f) as f:\n",
    "                        data = json.load(f)\n",
    "                except:\n",
    "                    pass\n",
    "                article_dict[file_id] = data\n",
    "\n",
    "        return article_dict\n",
    "\n",
    "\n",
    "    def extract_bodytext(self):\n",
    "        \"\"\"Unpack nested dictionary and extract body of the article\"\"\"\n",
    "        body = {}\n",
    "        article_dict = self.read_folder()\n",
    "        for k, v in article_dict.items():\n",
    "            strings = ''\n",
    "            prevString = ''\n",
    "            for entry in v['body_text']:\n",
    "                strings = strings + prevString\n",
    "                prevString = entry['text']\n",
    "\n",
    "            body[k] = strings\n",
    "        return body\n",
    "\n",
    "\n",
    "    def get_title_by_bodykv(self, article_dict, keyword):\n",
    "        \"\"\"Search keyword in article body and return title\"\"\"\n",
    "\n",
    "        article_dict = self.read_folder()\n",
    "        selected_id = self.extract_id_list()\n",
    "\n",
    "        result = {}\n",
    "        for k, v in article_dict.items():\n",
    "            for entry in v['body_text']:\n",
    "                if (keyword in entry['text'].split()) and (k in selected_id):\n",
    "                    result[k] = v['metadata']['title']\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def extract_id_list(self):\n",
    "        \"\"\"Extract ids from the selected text. \"\"\"\n",
    "        selected_id = []\n",
    "        for k, v in self.selected_id.items():\n",
    "            selected_id.append(str(v['sha']).split(';')[0])\n",
    "            try:\n",
    "                selected_id.append(str(v['sha']).split(';')[1])\n",
    "                selected_id.append(str(v['sha']).split(';')[2])\n",
    "                selected_id.append(str(v['sha']).split(';')[3])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return selected_id\n",
    "\n",
    "\n",
    "    def select_text_w_id(self):\n",
    "        body_text = self.extract_bodytext()\n",
    "        selected_id = self.extract_id_list()\n",
    "        selected_text = {}\n",
    "        for k, v in body_text.items():\n",
    "            if k in selected_id:\n",
    "                selected_text[k] = v\n",
    "        return selected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 Is wearing mask an effective way to control pandemic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
